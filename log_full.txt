C:\Github>cd whisper-ovep-python

C:\Github\whisper-ovep-python>dir
 Volume in drive C is OS
 Volume Serial Number is 801C-390E

 Directory of C:\Github\whisper-ovep-python

10/08/2025  11:24 AM    <DIR>          .
10/08/2025  11:25 AM    <DIR>          ..
10/08/2025  11:24 AM                66 .gitattributes
10/08/2025  11:24 AM    <DIR>          audio_files
10/08/2025  11:24 AM    <DIR>          config
10/08/2025  11:24 AM             4,779 dynamic_to_static.py
10/07/2025  05:37 AM    <DIR>          eval_dataset
10/08/2025  11:24 AM            15,650 README.md
10/08/2025  11:24 AM               109 requirements.txt
10/08/2025  11:24 AM            11,571 run_whisper.py
               5 File(s)         32,175 bytes
               5 Dir(s)  783,901,528,064 bytes free

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>python --version
Python 3.11.9

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>python -m pip install --upgrade pip
Requirement already satisfied: pip in c:\users\taroko\appdata\local\programs\python\python311\lib\site-packages (24.0)
Collecting pip
  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)
Using cached pip-25.2-py3-none-any.whl (1.8 MB)
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 24.0
    Uninstalling pip-24.0:
      Successfully uninstalled pip-24.0
Successfully installed pip-25.2

C:\Github\whisper-ovep-python>pip list
Package    Version
---------- -------
pip        25.2
setuptools 65.5.0

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>pip install -r requirements.txt
Collecting torch (from -r requirements.txt (line 1))
  Using cached torch-2.8.0-cp311-cp311-win_amd64.whl.metadata (30 kB)
Collecting torchaudio (from -r requirements.txt (line 2))
  Using cached torchaudio-2.8.0-cp311-cp311-win_amd64.whl.metadata (7.2 kB)
Collecting sounddevice (from -r requirements.txt (line 3))
  Using cached sounddevice-0.5.2-py3-none-win_amd64.whl.metadata (1.6 kB)
Collecting transformers==4.52.4 (from -r requirements.txt (line 4))
  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)
Collecting onnxsim (from -r requirements.txt (line 5))
  Using cached onnxsim-0.4.36-cp311-cp311-win_amd64.whl.metadata (4.4 kB)
Collecting optimum (from -r requirements.txt (line 6))
  Using cached optimum-1.27.0-py3-none-any.whl.metadata (16 kB)
Collecting accelerate (from -r requirements.txt (line 7))
  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)
Collecting jiwer (from -r requirements.txt (line 8))
  Using cached jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)
Collecting soundfile (from -r requirements.txt (line 9))
  Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)
Collecting onnx (from -r requirements.txt (line 10))
  Using cached onnx-1.19.0-cp311-cp311-win_amd64.whl.metadata (7.2 kB)
Collecting onnxruntime (from -r requirements.txt (line 11))
  Downloading onnxruntime-1.23.1-cp311-cp311-win_amd64.whl.metadata (5.2 kB)
Collecting filelock (from transformers==4.52.4->-r requirements.txt (line 4))
  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)
Collecting huggingface-hub<1.0,>=0.30.0 (from transformers==4.52.4->-r requirements.txt (line 4))
  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)
Collecting numpy>=1.17 (from transformers==4.52.4->-r requirements.txt (line 4))
  Using cached numpy-2.3.3-cp311-cp311-win_amd64.whl.metadata (60 kB)
Collecting packaging>=20.0 (from transformers==4.52.4->-r requirements.txt (line 4))
  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
Collecting pyyaml>=5.1 (from transformers==4.52.4->-r requirements.txt (line 4))
  Using cached pyyaml-6.0.3-cp311-cp311-win_amd64.whl.metadata (2.4 kB)
Collecting regex!=2019.12.17 (from transformers==4.52.4->-r requirements.txt (line 4))
  Using cached regex-2025.9.18-cp311-cp311-win_amd64.whl.metadata (41 kB)
Collecting requests (from transformers==4.52.4->-r requirements.txt (line 4))
  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)
Collecting tokenizers<0.22,>=0.21 (from transformers==4.52.4->-r requirements.txt (line 4))
  Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)
Collecting safetensors>=0.4.3 (from transformers==4.52.4->-r requirements.txt (line 4))
  Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)
Collecting tqdm>=4.27 (from transformers==4.52.4->-r requirements.txt (line 4))
  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4->-r requirements.txt (line 4))
  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)
Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.30.0->transformers==4.52.4->-r requirements.txt (line 4))
  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Collecting sympy>=1.13.3 (from torch->-r requirements.txt (line 1))
  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)
Collecting networkx (from torch->-r requirements.txt (line 1))
  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)
Collecting jinja2 (from torch->-r requirements.txt (line 1))
  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting CFFI>=1.0 (from sounddevice->-r requirements.txt (line 3))
  Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)
Collecting rich (from onnxsim->-r requirements.txt (line 5))
  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)
Collecting psutil (from accelerate->-r requirements.txt (line 7))
  Using cached psutil-7.1.0-cp37-abi3-win_amd64.whl.metadata (23 kB)
Collecting click>=8.1.8 (from jiwer->-r requirements.txt (line 8))
  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)
Collecting rapidfuzz>=3.9.7 (from jiwer->-r requirements.txt (line 8))
  Using cached rapidfuzz-3.14.1-cp311-cp311-win_amd64.whl.metadata (12 kB)
Collecting protobuf>=4.25.1 (from onnx->-r requirements.txt (line 10))
  Using cached protobuf-6.32.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)
Collecting ml_dtypes (from onnx->-r requirements.txt (line 10))
  Using cached ml_dtypes-0.5.3-cp311-cp311-win_amd64.whl.metadata (9.2 kB)
Collecting coloredlogs (from onnxruntime->-r requirements.txt (line 11))
  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)
Collecting flatbuffers (from onnxruntime->-r requirements.txt (line 11))
  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)
Collecting pycparser (from CFFI>=1.0->sounddevice->-r requirements.txt (line 3))
  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)
Collecting colorama (from click>=8.1.8->jiwer->-r requirements.txt (line 8))
  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)
Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->-r requirements.txt (line 1))
  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->-r requirements.txt (line 11))
  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)
Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime->-r requirements.txt (line 11))
  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)
Collecting MarkupSafe>=2.0 (from jinja2->torch->-r requirements.txt (line 1))
  Using cached markupsafe-3.0.3-cp311-cp311-win_amd64.whl.metadata (2.8 kB)
Collecting charset_normalizer<4,>=2 (from requests->transformers==4.52.4->-r requirements.txt (line 4))
  Using cached charset_normalizer-3.4.3-cp311-cp311-win_amd64.whl.metadata (37 kB)
Collecting idna<4,>=2.5 (from requests->transformers==4.52.4->-r requirements.txt (line 4))
  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.52.4->-r requirements.txt (line 4))
  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)
Collecting certifi>=2017.4.17 (from requests->transformers==4.52.4->-r requirements.txt (line 4))
  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)
Collecting markdown-it-py>=2.2.0 (from rich->onnxsim->-r requirements.txt (line 5))
  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)
Collecting pygments<3.0.0,>=2.13.0 (from rich->onnxsim->-r requirements.txt (line 5))
  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)
Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->onnxsim->-r requirements.txt (line 5))
  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)
Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)
Using cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)
Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)
Using cached torch-2.8.0-cp311-cp311-win_amd64.whl (241.4 MB)
Using cached torchaudio-2.8.0-cp311-cp311-win_amd64.whl (2.5 MB)
Using cached sounddevice-0.5.2-py3-none-win_amd64.whl (363 kB)
Using cached onnxsim-0.4.36-cp311-cp311-win_amd64.whl (1.3 MB)
Using cached optimum-1.27.0-py3-none-any.whl (425 kB)
Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)
Using cached numpy-2.3.3-cp311-cp311-win_amd64.whl (13.1 MB)
Using cached jiwer-4.0.0-py3-none-any.whl (23 kB)
Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)
Using cached onnx-1.19.0-cp311-cp311-win_amd64.whl (16.5 MB)
Downloading onnxruntime-1.23.1-cp311-cp311-win_amd64.whl (13.5 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13.5/13.5 MB 10.4 MB/s  0:00:01
Using cached cffi-2.0.0-cp311-cp311-win_amd64.whl (182 kB)
Using cached click-8.3.0-py3-none-any.whl (107 kB)
Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)
Using cached packaging-25.0-py3-none-any.whl (66 kB)
Using cached protobuf-6.32.1-cp310-abi3-win_amd64.whl (435 kB)
Using cached pyyaml-6.0.3-cp311-cp311-win_amd64.whl (158 kB)
Using cached rapidfuzz-3.14.1-cp311-cp311-win_amd64.whl (1.5 MB)
Using cached regex-2025.9.18-cp311-cp311-win_amd64.whl (276 kB)
Using cached safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)
Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)
Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)
Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)
Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)
Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)
Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)
Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)
Using cached filelock-3.19.1-py3-none-any.whl (15 kB)
Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)
Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)
Using cached markupsafe-3.0.3-cp311-cp311-win_amd64.whl (15 kB)
Using cached ml_dtypes-0.5.3-cp311-cp311-win_amd64.whl (206 kB)
Using cached networkx-3.5-py3-none-any.whl (2.0 MB)
Using cached psutil-7.1.0-cp37-abi3-win_amd64.whl (247 kB)
Using cached pycparser-2.23-py3-none-any.whl (118 kB)
Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)
Using cached requests-2.32.5-py3-none-any.whl (64 kB)
Using cached charset_normalizer-3.4.3-cp311-cp311-win_amd64.whl (107 kB)
Using cached idna-3.10-py3-none-any.whl (70 kB)
Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)
Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)
Using cached rich-14.1.0-py3-none-any.whl (243 kB)
Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)
Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)
Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)
Installing collected packages: mpmath, flatbuffers, urllib3, typing-extensions, sympy, safetensors, regex, rapidfuzz, pyyaml, pyreadline3, pygments, pycparser, psutil, protobuf, packaging, numpy, networkx, mdurl, MarkupSafe, idna, fsspec, filelock, colorama, charset_normalizer, certifi, tqdm, requests, ml_dtypes, markdown-it-py, jinja2, humanfriendly, click, CFFI, torch, soundfile, sounddevice, rich, onnx, jiwer, huggingface-hub, coloredlogs, torchaudio, tokenizers, onnxsim, onnxruntime, accelerate, transformers, optimum
Successfully installed CFFI-2.0.0 MarkupSafe-3.0.3 accelerate-1.10.1 certifi-2025.10.5 charset_normalizer-3.4.3 click-8.3.0 colorama-0.4.6 coloredlogs-15.0.1 filelock-3.19.1 flatbuffers-25.9.23 fsspec-2025.9.0 huggingface-hub-0.35.3 humanfriendly-10.0 idna-3.10 jinja2-3.1.6 jiwer-4.0.0 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.3 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 onnx-1.19.0 onnxruntime-1.23.1 onnxsim-0.4.36 optimum-1.27.0 packaging-25.0 protobuf-6.32.1 psutil-7.1.0 pycparser-2.23 pygments-2.19.2 pyreadline3-3.5.4 pyyaml-6.0.3 rapidfuzz-3.14.1 regex-2025.9.18 requests-2.32.5 rich-14.1.0 safetensors-0.6.2 sounddevice-0.5.2 soundfile-0.13.1 sympy-1.14.0 tokenizers-0.21.4 torch-2.8.0 torchaudio-2.8.0 tqdm-4.67.1 transformers-4.52.4 typing-extensions-4.15.0 urllib3-2.5.0

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>optimum-cli export onnx --model openai/whisper-base.en --opset 17 exported_whisper_base
C:\Users\Taroko\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\onnx\_internal\registration.py:162: OnnxExporterWarning: Symbolic function 'aten::scaled_dot_product_attention' already registered for opset 14. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.
  warnings.warn(
config.json: 1.94kB [00:00, ?B/s]
C:\Users\Taroko\AppData\Local\Programs\Python\Python311\Lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Taroko\.cache\huggingface\hub\models--openai--whisper-base.en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
  warnings.warn(message)
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290M/290M [00:15<00:00, 18.7MB/s]
generation_config.json: 1.53kB [00:00, ?B/s]
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 805/805 [00:00<?, ?B/s]
vocab.json: 798kB [00:00, 2.93MB/s]
tokenizer.json: 2.41MB [00:00, 21.0MB/s]
merges.txt: 456kB [00:00, 32.0MB/s]
normalizer.json: 52.7kB [00:00, 6.87MB/s]
added_tokens.json: 34.6kB [00:00, ?B/s]
special_tokens_map.json: 1.83kB [00:00, ?B/s]
preprocessor_config.json: 185kB [00:00, 91.3MB/s]
Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.
C:\Users\Taroko\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\models\whisper\modeling_whisper.py:881: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if input_features.shape[-1] != expected_seq_length:
C:\Users\Taroko\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\models\whisper\modeling_whisper.py:551: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):
Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
C:\Users\Taroko\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\models\whisper\modeling_whisper.py:1341: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if sequence_length != 1:
C:\Users\Taroko\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\cache_utils.py:556: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  or not self.key_cache[layer_idx].numel()  # the layer has no cache
C:\Users\Taroko\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\cache_utils.py:539: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  elif (
Found different candidate ONNX initializers (likely duplicate) for the tied weights:
        model.decoder.embed_tokens.weight: {'model.decoder.embed_tokens.weight'}
        proj_out.weight: {'onnx::MatMul_1642'}
Found different candidate ONNX initializers (likely duplicate) for the tied weights:
        model.decoder.embed_tokens.weight: {'model.decoder.embed_tokens.weight'}
        proj_out.weight: {'onnx::MatMul_1471'}

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>python dynamic_to_static.py --input_model_dir exported_whisper_base
[Converting Encoder...Begin]
WARNING: You might have to comment out ONNX checker in //onnxruntime/tools/onnx_model_utils.py if model > 2GB
Running: C:\Users\Taroko\AppData\Local\Programs\Python\Python311\python.exe -m onnxruntime.tools.make_dynamic_shape_fixed exported_whisper_base\encoder_model.onnx tmp\encoder_model.onnx --dim_param batch_size --dim_value 1
Running: C:\Users\Taroko\AppData\Local\Programs\Python\Python311\python.exe -m onnxruntime.tools.make_dynamic_shape_fixed tmp\encoder_model.onnx tmp\encoder_model.onnx --dim_param encoder_sequence_length / 2 --dim_value 1500
Running: C:\Users\Taroko\AppData\Local\Programs\Python\Python311\python.exe -m onnxruntime.tools.make_dynamic_shape_fixed tmp\encoder_model.onnx tmp\encoder_model.onnx --dim_param decoder_sequence_length --dim_value 180
Static conversion complete.
Inferencing shapes for: tmp\encoder_model.onnx
ONNX model is valid.
Shape inference complete. Overwritten: exported_whisper_base\encoder_model_static.onnx
---------- Running forward pass ----------------------
Generating dummy data for: input_features
Deleted temporary directory: tmp
[Converting Encoder...End]
[Converting Decoder...Begin]
Directory 'tmp' already exists. It has been deleted.
WARNING: You might have to comment out ONNX checker in //onnxruntime/tools/onnx_model_utils.py if model > 2GB
Running: C:\Users\Taroko\AppData\Local\Programs\Python\Python311\python.exe -m onnxruntime.tools.make_dynamic_shape_fixed exported_whisper_base\decoder_model.onnx tmp\decoder_model.onnx --dim_param batch_size --dim_value 1
Running: C:\Users\Taroko\AppData\Local\Programs\Python\Python311\python.exe -m onnxruntime.tools.make_dynamic_shape_fixed tmp\decoder_model.onnx tmp\decoder_model.onnx --dim_param encoder_sequence_length / 2 --dim_value 1500
Running: C:\Users\Taroko\AppData\Local\Programs\Python\Python311\python.exe -m onnxruntime.tools.make_dynamic_shape_fixed tmp\decoder_model.onnx tmp\decoder_model.onnx --dim_param decoder_sequence_length --dim_value 180
Static conversion complete.
Inferencing shapes for: tmp\decoder_model.onnx
ONNX model is valid.
Shape inference complete. Overwritten: exported_whisper_base\decoder_model_static.onnx
---------- Running forward pass ----------------------
Generating dummy data for: input_ids
Generating dummy data for: encoder_hidden_states
Deleted temporary directory: tmp
[Converting Decoder...End]
Model conversion successful.

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>curl -o ov_2025_1.zip https://storage.openvinotoolkit.org/repositories/openvino/packages/2025.1/windows/openvino_toolkit_windows_2025.1.0.18503.6fec06580ab_x86_64.zip
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  115M  100  115M    0     0  13.2M      0  0:00:08  0:00:08 --:--:-- 19.6M

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>tar -zxf ov_2025_1.zip

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>dir
 Volume in drive C is OS
 Volume Serial Number is 801C-390E

 Directory of C:\Github\whisper-ovep-python

10/08/2025  02:41 PM    <DIR>          .
10/08/2025  11:25 AM    <DIR>          ..
10/08/2025  11:24 AM                66 .gitattributes
10/08/2025  11:24 AM    <DIR>          audio_files
10/08/2025  11:24 AM    <DIR>          config
10/08/2025  11:24 AM             4,779 dynamic_to_static.py
10/07/2025  05:37 AM    <DIR>          eval_dataset
10/08/2025  02:39 PM    <DIR>          exported_whisper_base
10/08/2025  02:41 PM    <DIR>          openvino_toolkit_windows_2025.1.0.18503.6fec06580ab_x86_64
10/08/2025  02:41 PM       120,804,235 ov_2025_1.zip
10/08/2025  11:24 AM            15,650 README.md
10/08/2025  11:24 AM               109 requirements.txt
10/08/2025  11:24 AM            11,571 run_whisper.py
10/08/2025  02:39 PM    <DIR>          tmp
               6 File(s)    120,836,410 bytes
               8 Dir(s)  779,914,534,912 bytes free

C:\Github\whisper-ovep-python>.\openvino_toolkit_windows_2025.1.0.18503.6fec06580ab_x86_64\setupvars.bat
Python 3.11.9
[setupvars.bat] OpenVINO environment initialized

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>pip install onnxruntime-openvino
Collecting onnxruntime-openvino
  Downloading onnxruntime_openvino-1.22.0-cp311-cp311-win_amd64.whl.metadata (4.8 kB)
Requirement already satisfied: coloredlogs in c:\users\taroko\appdata\local\programs\python\python311\lib\site-packages (from onnxruntime-openvino) (15.0.1)
Requirement already satisfied: flatbuffers in c:\users\taroko\appdata\local\programs\python\python311\lib\site-packages (from onnxruntime-openvino) (25.9.23)
Requirement already satisfied: numpy>=1.21.6 in c:\users\taroko\appdata\local\programs\python\python311\lib\site-packages (from onnxruntime-openvino) (2.3.3)
Requirement already satisfied: packaging in c:\users\taroko\appdata\local\programs\python\python311\lib\site-packages (from onnxruntime-openvino) (25.0)
Requirement already satisfied: protobuf in c:\users\taroko\appdata\local\programs\python\python311\lib\site-packages (from onnxruntime-openvino) (6.32.1)
Requirement already satisfied: sympy in c:\users\taroko\appdata\local\programs\python\python311\lib\site-packages (from onnxruntime-openvino) (1.14.0)
Requirement already satisfied: humanfriendly>=9.1 in c:\users\taroko\appdata\local\programs\python\python311\lib\site-packages (from coloredlogs->onnxruntime-openvino) (10.0)
Requirement already satisfied: pyreadline3 in c:\users\taroko\appdata\local\programs\python\python311\lib\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime-openvino) (3.5.4)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\taroko\appdata\local\programs\python\python311\lib\site-packages (from sympy->onnxruntime-openvino) (1.3.0)
Downloading onnxruntime_openvino-1.22.0-cp311-cp311-win_amd64.whl (12.2 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12.2/12.2 MB 1.9 MB/s  0:00:06
Installing collected packages: onnxruntime-openvino
Successfully installed onnxruntime-openvino-1.22.0

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>python run_whisper.py --model-dir exported_whisper_base --device cpu --input audio_files/61-52s.wav
Selected provider: ['CPUExecutionProvider']
Provider option: None
C:\Users\Taroko\AppData\Local\Programs\Python\Python311\Lib\site-packages\torchaudio\_backend\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.
  warnings.warn(

Performance Metric (Chunk 1):
 Time to First Token for this chunk: 1.19 seconds

Performance Metric (Chunk 2):
 Time to First Token for this chunk: 50.57 seconds
 RTF: 1.85

Transcription: Also, there was a stripling page who turned into a maze with so sweet a lady, sir. And in some manner I do think she died. But then the picture was gone as quickly as it came. Sister Nell, do you hear these mottles? Take your place and let us see what the crystal can show to you, like his not young master. Though I am an old man. With all rant the opening of the tent to see what might be a miss. But Master Will, who peeped out first, needed no more than one glance. Mistress Fitzsuth to the rear of the Ted cries of "A knotting ham! A knotting ham!" before them fled the stroller and his three sons, "Capless and terrible!" "What is that tumult and rioting?" cried out the squire, authoritatively, and he blew twice on the silver whistle which hung at his belt.

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>python run_whisper.py --model-dir exported_whisper_base --device gpu --input audio_files/61-52s.wav
Selected provider: ['OpenVINOExecutionProvider']
Provider option: [{'device_type': 'GPU'}]
C:\Users\Taroko\AppData\Local\Programs\Python\Python311\Lib\site-packages\torchaudio\_backend\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.
  warnings.warn(

Performance Metric (Chunk 1):
 Time to First Token for this chunk: 0.09 seconds

Performance Metric (Chunk 2):
 Time to First Token for this chunk: 2.52 seconds
 RTF: 0.09

Transcription: Also, there was a stripling page who turned into a maze with so sweet a lady, sir. And in some manner I do think she died. But then the picture was gone as quickly as it came. Sister Nell, do you hear these mottles? Take your place and let us see what the crystal can show to you, like his not young master. Though I am an old man. With all rant the opening of the tent to see what might be a miss. But Master Will, who peeped out first, needed no more than one glance. Mistress Fitzsuth to the rear of the Ted cries of "A knotting ham! A knotting ham!" before them fled the stroller and his three sons, "Capless and terrible!" "What is that tumult and rioting?" cried out the squire, authoritatively, and he blew twice on the silver whistle which hung at his belt.

C:\Github\whisper-ovep-python>python run_whisper.py --model-dir exported_whisper_base --device npu --input audio_files/61-52s.wav
Selected provider: ['OpenVINOExecutionProvider']
Provider option: [{'device_type': 'NPU', 'cache_dir': './cache'}]
C:\Users\Taroko\AppData\Local\Programs\Python\Python311\Lib\site-packages\torchaudio\_backend\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.
  warnings.warn(

Performance Metric (Chunk 1):
 Time to First Token for this chunk: 0.21 seconds

Performance Metric (Chunk 2):
 Time to First Token for this chunk: 2.81 seconds
 RTF: 0.10

Transcription: Also, there was a stripling page who turned into a maze with so sweet a lady, sir. And in some manner I do think she died. But then the picture was gone as quickly as it came. Sister Nell, do you hear these mottles? Take your place and let us see what the crystal can show to you, like his not young master. Though I am an old man. With all rant the opening of the tent to see what might be a miss. But Master Will, who peeped out first, needed no more than one glance. Mistress Fitzsooth to the rear of the Ted cries of "A knotting ham! A knotting ham!" before them fled the stroller and his three sons, "Capless and terrible!" "What is that tumult and rioting?" cried out the squire, authoritatively, and he blew twice on the silver whistle which hung at his belt.

C:\Github\whisper-ovep-python>python run_whisper.py --model-dir exported_whisper_base --device npu --input audio_files/61-52s.wav
Selected provider: ['OpenVINOExecutionProvider']
Provider option: [{'device_type': 'NPU', 'cache_dir': './cache'}]
C:\Users\Taroko\AppData\Local\Programs\Python\Python311\Lib\site-packages\torchaudio\_backend\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.
  warnings.warn(

Performance Metric (Chunk 1):
 Time to First Token for this chunk: 0.18 seconds

Performance Metric (Chunk 2):
 Time to First Token for this chunk: 2.80 seconds
 RTF: 0.10

Transcription: Also, there was a stripling page who turned into a maze with so sweet a lady, sir. And in some manner I do think she died. But then the picture was gone as quickly as it came. Sister Nell, do you hear these mottles? Take your place and let us see what the crystal can show to you, like his not young master. Though I am an old man. With all rant the opening of the tent to see what might be a miss. But Master Will, who peeped out first, needed no more than one glance. Mistress Fitzsooth to the rear of the Ted cries of "A knotting ham! A knotting ham!" before them fled the stroller and his three sons, "Capless and terrible!" "What is that tumult and rioting?" cried out the squire, authoritatively, and he blew twice on the silver whistle which hung at his belt.

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>python run_whisper.py --model-dir exported_whisper_base --device npu --input mic
Selected provider: ['OpenVINOExecutionProvider']
Provider option: [{'device_type': 'NPU', 'cache_dir': './cache'}]

ðŸŽ¤ Real-time Transcription. Start Speaking ..

Hello
Hello? Hello?
How are you today?



ðŸ”• Silence detected. Stopping transcription.

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>python run_whisper.py --model-dir exported_whisper_base --device cpu --eval-dir eval_dataset\LibriSpeech-samples
Selected provider: ['CPUExecutionProvider']
Provider option: None
C:\Users\Taroko\AppData\Local\Programs\Python\Python311\Lib\site-packages\torchaudio\_backend\utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.
  warnings.warn(

Performance Metric (Chunk 1):
 Time to First Token for this chunk: 1.62 seconds
 RTF: 1.72

Performance Metric (Chunk 1):
 Time to First Token for this chunk: 1.55 seconds
 RTF: 1.91

Performance Metric (Chunk 1):
 Time to First Token for this chunk: 1.49 seconds
 RTF: 2.21
Evaluation completed for 3 files.
Average WER: 0.153, Average CER: 0.030, Average RTF: 1.948

C:\Github\whisper-ovep-python>
C:\Github\whisper-ovep-python>
